{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "dcacb0086e9a4f4eabd41c33bf4faac5ea0a3337ed3f5eff0680afa930572c04"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Topic: Time series analysis for predictive maintenance of turbofan engines.\n",
    "Goal: The project aim is to build a predictive model- used Linear regression - to estimate the Remaining Useful Life ( RUL) of a jet engine based on run-to-failure data of a fleet of similar jet engines.\n",
    "\n",
    "Data: the datasets consist of 12 files , 4 files for training the data , other 4 files for testig the model , and last 4 for the RUL(Remainging Useful Life) , The FD001 dataset is only considered out of the four datasets that are available. The .txt files that are present under FD001 and are detailedly explained below. train_FD001.txt: Contains 20631 data points from 100 different engines. Each datapoint consists of engine id, cycle (time), 3 operation settings (Operational settings: altitude, throttle resolver angle (TRA), Mach number\n",
    ") and 21 sensor measurements (consisting of speeds , temperature , and pressures). \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_FD001.txt', sep=\" \", header=None )\n",
    "test_df = pd.read_csv('test.txt', sep=\" \", header=None) \n",
    "RUL_df = pd.read_csv('RUL_FD001.txt', sep=\" \", header=None) \n",
    "train_df.head(5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data (2 extra columns were added due to white space, deleting those and adding the column names\n",
    "test_df = test_df[list(range(26))]\n",
    "train_df = train_df[list(range(26))]\n",
    "# column names for the dataset\n",
    "col_list = ['engine_num', 'time', 'op_cond_1', 'op_cond_2', 'op_cond_3', 'sm_1', 'sm_2', 'sm_3', 'sm_4', 'sm_5', 'sm_6', 'sm_7', 'sm_8', 'sm_9', 'sm_10', 'sm_11', 'sm_12', 'sm_13', 'sm_14', 'sm_15', 'sm_16', 'sm_17', 'sm_18', 'sm_19', 'sm_20', 'sm_21']\n",
    "# op_cond refers to operational condition, sn: sensor\n",
    "train_df.columns = col_list\n",
    "test_df.columns = col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "#all data are numerical and no missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the mean for all the columns \n",
    "train_df.mean().plot.bar(figsize=(12,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Examining the Correlations Between the features using the correlation heatmap\"\n",
    "corr_all=train_df.drop('time',axis=1).corr(method='pearson')# linear correlation between variables for all engines\n",
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "mask = np.zeros_like(corr_all)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(corr_all,mask=mask,cmap='RdYlGn',linewidths=0.2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(df, engine_num=None):\n",
    "    '''plot all non trivial measurements and states'''\n",
    "    cols = df.columns\n",
    "    n_cols = min(len(cols), 5)\n",
    "    n_rows = int(np.ceil(len(cols) / n_cols))\n",
    "    sns.set()\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15,12))\n",
    "    axes = axes.flatten()\n",
    "    if engine_num != None: \n",
    "        fig.suptitle('distributions for engine #: {}'.format(engine_num))\n",
    "        df_plot = df.loc[engine_num]\n",
    "    else: \n",
    "        fig.suptitle('distributions for all engines')\n",
    "        df_plot = df\n",
    "    for col, ax in zip(cols, axes):\n",
    "        ax=sns.distplot(df_plot[col], ax=ax, label=col)\n",
    "        ax.legend(loc=1)\n",
    "#         labels(col, \"p\", ax)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plot_dist(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Feature selection \n",
    "i'll be using two approaches to select the features to be used\n",
    "1- find high correlation column pairs in train_df using a function \n",
    "2- Plot the data of the 3 operational settings and 21 sensor measurements to check the trends of the engines from healthy state to failure state.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corr_pairs(corr,thrsh):\n",
    "    \"\"\"\n",
    "    find high correlation column pairs in df \n",
    "    ======================================\n",
    "    input: \n",
    "    corr - (df)- correlation matrix generated by pandas\n",
    "    thrsh - (float) threshold value to consider correlation as high so that it is included in the output \n",
    "    output: high_corr_pairs - (list) list of tuples of the two-column names and their correlation. corr> thrsh\n",
    "    \"\"\"\n",
    "    high_corr_pairs = []\n",
    "    # same as input 'corr' but the upper -triangle half of the matrix is zeros ( for convenience only) \n",
    "    corr_diag = pd.DataFrame(np.tril(corr.values), columns=corr.columns, index = corr.index)\n",
    "    # check  the correlation between every pair of columns in the corr and keeps the high ones\n",
    "    for col_num , col in enumerate(corr_diag):\n",
    "        col_corr=corr_diag[col].iloc[col_num+1:] # this slicing ensures ignoring self_corr and duplicates due to symmetry\n",
    "        # bool mask for pairs with high corr with col\n",
    "        mask_pairs = col_corr.apply(lambda x: abs(x))>thrsh \n",
    "        idx_pairs=col_corr[mask_pairs].index\n",
    "        # create list of high corr pairs\n",
    "        for idx , corr in zip(idx_pairs,col_corr[mask_pairs].values):\n",
    "            high_corr_pairs.append((col, idx, corr))\n",
    "    return high_corr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pairs=find_corr_pairs(corr_all,0.8)\n",
    "for c in corr_pairs:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, nrows =6, figsize=(24, 20))\n",
    "ax = ax.ravel()\n",
    "for i, item in enumerate(col_list[2:]):\n",
    "  train_df.groupby('engine_num').plot(kind='line', x = \"time\", y = item, ax=ax[i])\n",
    "  ax[i].get_legend().remove()\n",
    "  ax[i].title.set_text(item)\n",
    "plt.subplots_adjust(top = 0.99, bottom = 0.01, hspace = 0.3, wspace = 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" it's being noticed from above two approches that some feature ain't significant for our anaylsis \n",
    "1- based on first approach : sm_4', 'sm_11', 'sm_12','sm_7', 'sm_8', 'sm_13', 'sm_9', 'sm_14' are important \n",
    "2- based on second approach : 'op_cond_3', 'sm_1', 'sm_5', 'sm_10', 'sm_16' , 'sm_18', 'sm_19' are not important as they don't change their state in time \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_remove = ['op_cond_3', 'sm_1', 'sm_5', 'sm_10', 'sm_16' , 'sm_18', 'sm_19'] \n",
    "train_df.drop(columns=col_remove,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"this is the maximun cycles that each of the 100 different engines reached to before the data stopped(failure occured), so each max cycle (time) is considered to be the value of RUL of an engine\"\n",
    "train_df.groupby('engine_num')['time'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  each engine develops a fault, which can be seen through sensor readings. the data stops for each engine when a failure has occurred for that particular engine. hence the actual RUL is known based on the length of the data, RUL is simply the number of operational cycles after the last cycle that the engine will continue to operate properly\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Labeling - generate column RUL \n",
    "rul = pd.DataFrame(train_df.groupby('engine_num')['time'].max()).reset_index()\n",
    "rul.columns = ['engine_num', 'max']\n",
    "train_df = train_df.merge(rul, on=['engine_num'], how='left')\n",
    "train_df['RUL'] = train_df['max'] - train_df['time']\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "\"observed that whenerver the time(cycle) increases for a particular enginer , the RUL is inversely propotional with it as it's decreasing until it reaches 0 (failure)\"\n",
    "train_df.head(192)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_df.corr()\n",
    "print(corr['RUL'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting all the train data by sensors\n",
    "t = train_df.iloc[0:,2:-1].plot(subplots=True, figsize=(25, 17))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse the RUL for the plot\n",
    "train_df['RUL'] = train_df['RUL'] * -1\n",
    "# plot all engines sensor data\n",
    "g = sns.PairGrid(data=train_df, x_vars='RUL', y_vars=train_df.iloc[0:,2:-1], hue=\"engine_num\", height=2, aspect=6,)\n",
    "g = g.map(plt.plot)\n",
    "g = g.set(xlim=(train_df['RUL'].min(), train_df['RUL'].max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Prepararing the training and the testing dataset \"\n",
    "def get_train_test(train_df):\n",
    "  engine_nums = train_df.engine_num.unique()\n",
    "  np.random.seed(0)\n",
    "  engine_num_train = np.random.choice(engine_nums, int(len(engine_nums)*0.8), replace=False)\n",
    "  engine_num_test = np.array(list(set(engine_nums) - set(engine_num_train))) \n",
    "\n",
    "  x_train = train_df[train_df.engine_num==engine_num_train[0]]\n",
    "  y_train = train_df.RUL[train_df.engine_num==engine_num_train[0]]\n",
    "  for engine_num in engine_num_train[1:]:\n",
    "     x_train = pd.concat([x_train, train_df[train_df.engine_num==engine_num]])\n",
    "     y_train = pd.concat([y_train, train_df.RUL[train_df.engine_num==engine_num]])\n",
    "\n",
    "  x_test = train_df[train_df.engine_num==engine_num_test[0]]\n",
    "  y_test = train_df.RUL[train_df.engine_num==engine_num_test[0]]\n",
    "  for engine_num in engine_num_test[1:]:\n",
    "     x_test = pd.concat([x_test, train_df[train_df.engine_num==engine_num]])\n",
    "     y_test = pd.concat([y_test, train_df.RUL[train_df.engine_num==engine_num]])\n",
    "\n",
    "  x_train.drop(['engine_num', 'time', 'RUL'], axis=1, inplace=True)\n",
    "  x_test.drop(['engine_num', 'time', 'RUL'], axis=1, inplace=True)\n",
    "  return x_train, x_test, y_train, y_test\n",
    "x_train, x_test, y_train, y_test =get_train_test(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "reg_lin = LinearRegression()\n",
    "reg_lin.fit(x_train, y_train)\n",
    "predictions = reg_lin.predict(x_test)\n",
    "plt.figure()\n",
    "plt.scatter(y_test, predictions, s=5, alpha=0.15)\n",
    "plt.xlabel('RUL (true)')\n",
    "plt.ylabel('RUL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred, weights=None):\n",
    "  diff = np.array(y_pred, dtype=np.float) - np.array(y_true, dtype=np.float)\n",
    "  mask = diff < 0\n",
    "  diff[mask] = np.exp(-diff[mask]*0.077) - 1\n",
    "  mask = diff >= 0\n",
    "  diff[mask] = np.exp(diff[mask]*0.1) - 1\n",
    "  return diff.sum()\n",
    "\n",
    "predictions[predictions < 1] = 1\n",
    "print('MSE', mean_squared_error(y_test, predictions))\n",
    "print('R2 score', r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Reference:\n",
    "*https://www.mathworks.com/help/predmaint/ug/rul-estimation-using-rul-estimator-models.html\n",
    "*https://medium.com/@hamalyas_/\n",
    "*takashi matsushita github\n",
    "\"\"\""
   ]
  }
 ]
}
